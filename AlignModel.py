# -*- coding: utf-8 -*-
"""
The align class
"""
import numpy as np
from photonpy import Dataset
import tensorflow as tf
import copy
import matplotlib.pyplot as plt
from photonpy import PostProcessMethods, Context


from Align_Modules.Affine import AffineModel
from Align_Modules.Polynomial3 import Polynomial3Model
from Align_Modules.RigidBody import RigidBodyModel
from Align_Modules.Splines import CatmullRomSpline2D
from Align_Modules.Shift import ShiftModel
from Align_Datasets.channel_class import channel
from Plot import Plot



#%% Align class
class AlignModel(Plot):
    '''
    The AlignModel Class is a class used for the optimization of a certain Dataset class. Important is 
    that the loaded class contains the next variables:
        ch1, ch2, ch2_original : Nx2 float32
            The two channels (and original channel2) that both contain positions callable by .pos
        coupled : bool
            True if the dataset is coupled (points are one-to-one)
        img, imgsize, mid: np.array
            The image borders, size and midpoint. Generated by self.imgparams            
    '''
    def __init__(self, developer_mode=False):        
        ## Models
        self.AffineModel = None
        self.Polynomial3Model = None
        self.RigidBodyModel = None
        self.ShiftModel = None
        self.SplinesModel = None
        self.CP_locs = None
        self.gridsize=None
        self.edge_grids=None
        self.developer_mode=developer_mode
        
        ## Neighbours
        self.CP_idx_NN=None
        self.NN_maxDist=None
        self.NN_threshold=None
        self.NN_k=None
        self.Neighbours=False
        (self.NN1, self.NN2)=(None,None)
        
        Plot.__init__(self)
            
            
    def dev_mode(self, algorithm=None):
    # To check the parameters during simulation
        if self.developer_mode:
            #plt.close('all')
            _,_,fig,_=self.ErrorPlot(nbins=30)
            if algorithm is not None: fig.suptitle(algorithm, fontweight='bold', fontsize = 20)
            '''
            plt.pause(1)
            while True:
                if algorithm is not None: print('\nDeveloper Mode for', algorithm)    
                inp = input('Correct Parameters? [y/n]:')
                if inp == 'y':
                    continue
                elif inp == 'n':
                    sys.exit('Exit via developer mode')
                else:
                    print('Invalid input')
            plt.close('all')
            '''
                
        
    # common functions (also callabel via the Dataset classes)
    def imgparams(self):
    # calculate borders of system
    # returns a 2x2 matrix containing the edges of the image, a 2-vector containing
    # the size of the image and a 2-vector containing the middle of the image
        img1 = np.empty([2,2, self.Nbatch], dtype = float)
        for batch in range(self.Nbatch):
            img1[0,0,batch] = np.min(( np.min(self.ch1[batch].pos[:,0]), np.min(self.ch2[batch].pos[:,0]) ))
            img1[1,0,batch] = np.max(( np.max(self.ch1[batch].pos[:,0]), np.max(self.ch2[batch].pos[:,0]) ))
            img1[0,1,batch] = np.min(( np.min(self.ch1[batch].pos[:,1]), np.min(self.ch2[batch].pos[:,1]) ))
            img1[1,1,batch] = np.max(( np.max(self.ch1[batch].pos[:,1]), np.max(self.ch2[batch].pos[:,1]) ))
        
        img = np.empty([2,2], dtype = float)
        img[0,0] = np.min(img1[0,0,:])
        img[1,0] = np.max(img1[1,0,:])
        img[0,1] = np.min(img1[0,1,:])
        img[1,1] = np.max(img1[1,1,:])
        return img, (img[1,:] - img[0,:]), (img[1,:] + img[0,:])/2
    
    
    def couple_dataset(self, FrameLinking=False):
    # couples dataset with a simple iterative nearest neighbour method
    # FrameLinking links the dataset per frame
        print('Coupling datasets with an iterative method...')
        for batch in range(self.Nbatch):
            (locsB,frameB)=([],[])
            for i in range(self.ch1[batch].pos.shape[0]):
                if FrameLinking:
                    sameframe_pos = self.ch2[batch].pos[self.ch2[batch].frame==self.ch1[batch].frame[i],:]
                    dists = np.sqrt(np.sum((self.ch1[batch].pos[i,:]-sameframe_pos)**2,1))
                
                    iB=np.argmin(dists)
                    locsB.append(sameframe_pos[iB,:])
                    frameB.append(self.ch1[batch].frame[i])
                else:
                    dists = np.sqrt(np.sum((self.ch1[batch].pos[i,:]-self.ch2[batch].pos)**2,1))
                    iB=np.argmin(dists)
                    locsB.append(self.ch2[batch].pos[iB,:])
                    frameB.append(self.ch2[batch].frame[iB])
            
            self.ch2[batch].pos = np.array(locsB)
            self.ch2[batch].frame = np.array(frameB)
            
        self.coupled = True
        
        
    def Filter_Pairs(self, maxDist=150):
    # Filter pairs above maxDist
        print('Filtering pairs above',maxDist,'nm...')
        if not self.coupled: raise Exception('Dataset should be coupled before filtering pairs')
        
        for batch in range(self.Nbatch):
            dists = np.sqrt(np.sum( (self.ch1[batch].pos - self.ch2[batch].pos)**2 ,axis=1))
            idx = np.argwhere(dists<maxDist)
            
            if idx.shape[0]==0: raise ValueError('All localizations will be filtered out in current settings.')
            self.ch1[batch].pos = np.squeeze(self.ch1[batch].pos[idx,:],axis=1)
            self.ch2[batch].pos = np.squeeze(self.ch2[batch].pos[idx,:],axis=1)
            if self.ch2_original[batch] is not None: self.ch2_original[batch].pos = np.squeeze(self.ch2_original[batch].pos[idx,:],axis=1)
        self.dev_mode('Filter Pairs')
        
        
    def copy_models(self, other):
        self.AffineModel = copy.deepcopy(other.AffineModel)
        self.Polynomial3Model = copy.deepcopy(other.Polynomial3Model)
        self.RigidBodyModel = copy.deepcopy(other.RigidBodyModel)
        self.ShiftModel = copy.deepcopy(other.ShiftModel)
        self.SplinesModel = copy.deepcopy(other.SplinesModel)
        
        self.CP_locs = copy.deepcopy(other.CP_locs)
        self.gridsize = other.gridsize
        self.edge_grids = other.edge_grids
        if self.gridsize is not None:
            self.x1_min = other.x1_min
            self.x2_min = other.x2_min
            self.x1_max = other.x1_max
            self.x2_max = other.x2_max
        
            
    #%% Split dataset or load subset
    def SubsetWindow(self, subset):
    # loading subset of dataset by creating a window of size subset 
        if self.Nbatch>1: raise ValueError('SubsetRandom should be used before creating batches!')
        l_grid = self.mid - np.array([ subset*self.imgsize[0], subset*self.imgsize[1] ])/2
        r_grid = self.mid + np.array([ subset*self.imgsize[0], subset*self.imgsize[1] ])/2
    
        mask1 = np.where( (self.ch1[0].pos[:,0] >= l_grid[0]) * (self.ch1[0].pos[:,1] >= l_grid[1])
                            * (self.ch1[0].pos[:,0] <= r_grid[0]) * (self.ch1[0].pos[:,1] <= r_grid[1]) , True, False)
        mask2 = np.where( (self.ch2[0].pos[:,0] >= l_grid[0]) * (self.ch2[0].pos[:,1] >= l_grid[1])
                            * (self.ch2[0].pos[:,0] <= r_grid[0]) * (self.ch2[0].pos[:,1] <= r_grid[1]), True, False )

        self = self.gather(mask1, mask2)
        
        
    def SubsetRandom(self, subset):
    # loading subset of dataset by taking a random subset
        if self.Nbatch>1: raise ValueError('SubsetRandom should be used before creating batches!')
        if self.coupled:
            mask1=np.random.choice(self.ch1[0].pos.shape[0], int(self.ch1[0].pos.shape[0]*subset))
            mask2=mask1
        else:
            mask1=np.random.choice(self.ch1[0].pos.shape[0], int(self.ch1[0].pos.shape[0]*subset))
            mask2=np.random.choice(self.ch2[0].pos.shape[0], int(self.ch2[0].pos.shape[0]*subset))
            
        self = self.gather(mask1, mask2)
        
        
    def SplitDataset(self):
    # Splits dataset into 2 halves for cross validation
        if self.Nbatch>1: raise ValueError('SplitDataset should be used before creating batches!')
        if self.Neighbours: print('WARNING: splitting datasets means the neighbours need to be reloaded!')
        
        N1=self.ch1[0].pos[0].shape[0]
        N2=self.ch2[0].pos[0].shape[0]
        if self.coupled:
            if N1!=N2: raise Exception('Datasets are coupled but not equal in size')
            mask1=np.ones(N1, dtype=bool)
            mask1[int(N1/2):]=False
            np.random.shuffle(mask1)  # create random mask to split dataset in two
            mask2 = np.abs(mask1-1).astype('bool')

            other1=self.gather(mask1, mask1)
            other2=self.gather(mask2, mask2)
            
        else:
            mask11=np.ones(N1, dtype=bool)
            mask11[int(N1/2):]=False
            mask12=np.ones(N2, dtype=bool)
            mask12[int(N2/2):]=False
            np.random.shuffle(mask11)  # create random mask to split dataset in two
            np.random.shuffle(mask12)
            mask21 = np.abs(mask11-1).astype('bool')
            mask22 = np.abs(mask12-1).astype('bool')
            
            other1=self.gather(mask11, mask12)
            other2=self.gather(mask21, mask22)
        
        return other1, other2
    
    
    def SplitFrames(self):
        print('Splitting Dataset into different frames...')
        if self.Nbatch>1: raise ValueError('Dataset already split in batches')
        frames = np.unique(self.ch1[0].frame)
        (ch1, ch2, ch2_original) = ([],[],[])
        for frame in frames:
            idx1 = np.argwhere(self.ch1[0].frame==frame)
            idx2 = np.argwhere(self.ch2[0].frame==frame)
            
            ch1.append( channel(pos=np.squeeze(self.ch1[0].pos[idx1,:], axis=1), _xyI = self.ch1[0]._xyI()[idx1,:], frame = self.ch1[0].frame[idx1]) )
            ch2.append( channel(pos=np.squeeze(self.ch2[0].pos[idx1,:], axis=1), _xyI = self.ch2[0]._xyI()[idx2,:], frame = self.ch2[0].frame[idx2]) )
            ch2_original.append( channel(pos=np.squeeze(self.ch2_original[0].pos[idx1,:], axis=1), _xyI = self.ch2_original[0]._xyI()[idx2,:], frame = self.ch2_original[0].frame[idx2]) )
            
        self.ch1=ch1
        self.ch2=ch2
        self.ch2_original=ch2_original
        self.Nbatch = len(self.ch1)
    
    def gather(self, idx1, idx2):
    # gathers the indexes of both channels
        other = copy.deepcopy(self)
        
        del other.ch1, other.ch2, other.ch2_original
        (ch1, ch2, ch2_original) = ([],[],[])
        for batch in range(self.Nbatch):
            ch1.append( channel(pos=self.ch1.pos[idx1,:], _xyI = self.ch1._xyI()[idx1,:], frame = self.ch1.frame[idx1]) )
            ch2.append( channel(pos=self.ch2.pos[idx2,:], _xyI = self.ch2._xyI()[idx2,:], frame = self.ch2.frame[idx2]) )
            ch2_original.append( channel(pos=self.ch2_original.pos[idx2,:], _xyI = self.ch2_original._xyI()[idx2,:], frame = self.ch2.frame[idx2]) )
        
        other.ch1=ch1
        other.ch2=ch2
        other.ch2_original=ch2_original
        return other
    
    
    def concat_batches(self):
        (ch1_lst, ch2_lst, ch2_original_lst, f1_lst, f2_lst, f2_original_lst) = ([],[],[],[],[],[])
        for batch in range(self.Nbatch):
            ch1_lst.append(self.ch1[batch].pos)
            ch2_lst.append(self.ch2[batch].pos)
            ch2_original_lst.append(self.ch2_original[batch].pos)
            f1_lst.append(self.ch1[batch].frame)
            f2_lst.append(self.ch2[batch].frame)
            f2_original_lst.append(self.ch2_original[batch].frame)
            
        self.ch1= channel(self.imgshape, pos = np.concatenate(ch1_lst, axis=0), frame = np.concatenate(f1_lst, axis=0))
        self.ch2= channel(self.imgshape, pos = np.concatenate(ch2_lst, axis=0), frame = np.concatenate(f2_lst, axis=0))
        self.ch2_original= channel(self.imgshape, pos = np.concatenate(ch2_original_lst, axis=0), frame = np.concatenate(f2_original_lst, axis=0))
            
    
    #%% Generate Neighbours
    def find_neighbours(self, maxDistance=50, k=20):
    # Tries to generate neighbours according to brightest spots, and tries kNN otherwise
        print('Finding neighbours within a distance of',maxDistance,'nm for spots containing at least',k,'neighbours...')
        (self.NN1, self.NN2) = ([],[])
        for batch in range(self.Nbatch):
            try:
                NN1, NN2 = self.find_BrightNN(self.ch1[batch], self.ch2[batch], maxDistance=maxDistance, threshold=k)
            except Exception:
                print('Not enough bright Neighbours found in current setting. Switching to kNN with k = ',k,'!')
                NN1, NN2 = self.find_kNN(self.ch1[batch], self.ch2[batch], k)
            
            self.NN1.append(NN1)
            self.NN2.append(NN2)
        self.Neighbours=True
        
        
    def find_BrightNN(self, ch1, ch2, maxDistance = 50, threshold = 20):
    # generates the brightest neighbours
    # outputs a list of indices for the neigbhours
        with Context() as ctx: # loading all NN
            counts,indices = PostProcessMethods(ctx).FindNeighbors(ch1.pos, ch2.pos, maxDistance)
    
        ## putting all NNidx in a list 
        (idxlist, pos, i) = ([], 0,0)
        for count in counts:
            idxlist.append( np.stack([
                i * np.ones([count], dtype=int),
                indices[pos:pos+count] 
                ]) )
            pos+=count
            i+=1
            
        ## filtering the NNidx list to be square
        (idx1list, idx2list) = ([], [])
        for idx in idxlist:
            if idx.size>0 and idx.shape[1] > threshold: # filter for brightest spots above threshold
                # select random sample from brightest spost
                idx1list.append(idx[0, np.random.choice(idx.shape[1], threshold)]) 
                idx2list.append(idx[1, np.random.choice(idx.shape[1], threshold)]) 
    
        ## look if neighbours actually have been found
        if idx1list==[]: 
            raise Exception('No neighbours were generated. Adjust the threshold or maxDistance!')
        else:
            self.NN_maxDist=maxDistance
            self.NN_threshold=threshold
            
        ## Loading the indexes as matrices
        return self.load_NN_matrix(ch1, ch2, idx1list, idx2list)
        
        
    def find_kNN(self, ch1, ch2, k):
    # generates the k-nearest neighbours
    # outputs a list of indices for the neigbhours
        (idx1list, idx2list) = ([],[])
        for i in range(ch1.pos.shape[0]):
            idx1list.append( (i * np.ones(k, dtype=int)) )
        for loc in ch1.pos:
            distances = np.sum((loc - ch2.pos)**2 , axis = 1)
            idx2list.append( np.argsort( distances )[:k] )
            
        self.NN_k=k
        
        ## Loading the indexes as matrices
        return self.load_NN_matrix(ch1, ch2, idx1list, idx2list)
    
    
    def load_NN_matrix(self,ch1, ch2, idx1list, idx2list):
    # Takes the indexes for channel 1 and 2 and loads the matrix
        (NN1, NN2) = ([],[])
        for nn in idx1list: NN1.append(ch1.pos[nn, :])
        for nn in idx2list: NN2.append(ch2.pos[nn, :])
        return np.stack(NN1, axis=0), np.stack(NN2, axis=0)
        
    
    #%% Optimization functions
    def loss_fn(self, ch1, ch2):
        if self.coupled:
            loss = ( tf.reduce_sum(tf.square(ch1-ch2)) )
        else: 
            CRLB = .15
            D_KL = 0.5*tf.reduce_sum( tf.square(ch1 - ch2) / CRLB**2 , axis=2)
            loss = ( -1*tf.math.log( tf.reduce_sum( tf.math.exp( -1*D_KL / ch2.shape[2] ) / ch2.shape[0] , axis = 1) ) ) 
        return loss
    

    def train_model(self, model, Nit, opt, pos1_tf=None, pos2_tf=None):
    # The training loop of the model
        if pos1_tf is None and pos2_tf is None:
            (pos1_tf, pos2_tf) = ([],[])
            if self.coupled:
                for batch in range(self.Nbatch):
                    pos1_tf.append( tf.Variable(self.ch1[batch].pos, dtype=tf.float32, trainable=False) )
                    pos2_tf.append( tf.Variable(self.ch2[batch].pos, dtype=tf.float32, trainable=False) )
            elif self.Neighbours:
                for batch in range(self.Nbatch):
                    pos1_tf.append( tf.Variable(self.NN1[batch], dtype=tf.float32, trainable=False) )
                    pos2_tf.append( tf.Variable(self.NN2[batch], dtype=tf.float32, trainable=False) )
            else:
                raise Exception('Dataset is not coupled but no Neighbours have been generated yet')
            
        
        for i in range(Nit):
            loss = 0
            with tf.GradientTape() as tape:
                for batch in range(self.Nbatch):
                    pos2_mapped = model(pos1_tf[batch], pos2_tf[batch])
                    loss += self.loss_fn(pos1_tf[batch], pos2_mapped)
                     
            grads = tape.gradient(loss, model.trainable_weights)
            opt.apply_gradients(zip(grads, model.trainable_weights))
        return model
            
    
    #%% Global Transforms (Affine, Polynomial3, RigidBody)
    ## Shift
    @tf.function
    def Train_Shift(self, lr=1, Nit=200):
    # Training the RigidBody Mapping
        #if self.ShiftModel is not None: raise Exception('Models can only be trained once')
        
        # initializing the model and optimizer
        self.ShiftModel=ShiftModel(direct=self.coupled)
        opt=tf.optimizers.Adagrad(lr)
        
        # Training the Model
        print('Training Shift Mapping (lr, #it) =',str((lr, Nit)),'...')
        self.ShiftModel = self.train_model(self.ShiftModel, Nit, opt)
        
    #@tf.function
    def Transform_Shift(self):
    # Transforms ch2 according to the Model
        print('Transforming Shift Mapping...')
        if self.ShiftModel is None: print('Model not trained yet, will pass without transforming.')
        else:
            for batch in range(self.Nbatch):
                ch2_tf=tf.Variable(self.ch2[batch].pos, dtype=tf.float32, trainable=False)
                ch2_tf=self.ShiftModel.transform_vec(ch2_tf)
                self.ch2[batch].pos=np.array(ch2_tf.numpy())
                if np.isnan( self.ch2[batch].pos ).any(): raise ValueError('ch2 contains infinities. The Shift mapping likely exploded.')
        self.dev_mode('Shift')
    
    
    ## RigidBody
    def Train_RigidBody(self, lr=1, Nit=200):
    # Training the RigidBody Mapping
        if self.RidigBodyModel is not None: raise Exception('Models can only be trained once')
        if self.mid.all() != 0: print('WARNING! The image is not centered. This may have have detrimental effects for mapping a rotation!')
        
        # initializing the model and optimizer
        self.RigidBodyModel=RigidBodyModel(direct=self.coupled)
        opt=tf.optimizers.Adagrad(lr)
        
        # Training the Model
        print('Training RigidBody Mapping (lr, #it) =',str((lr, Nit)),'...')
        self.RigidBodyModel = self.train_model(self.RigidBodyModel, Nit, opt)
        
    
    def Transform_RigidBody(self):
    # Transforms ch2 according to the Model
        if self.RigidBodyModel is None: print('Model not trained yet, will pass without transforming.')
        else:
            if self.mid.all() != 0: print('WARNING! The image is not centered. This may have have detrimental effects for mapping a rotation!')
            for batch in range(self.Nbatch):
                print('Transforming RigidBody Mapping...')
                ch2_tf=tf.Variable(self.ch2[batch].pos, dtype=tf.float32, trainable=False)
                ch2_tf=self.RigidBodyModel.transform_vec(ch2_tf)
                self.ch2[batch].pos=np.array(ch2_tf.numpy())
                if np.isnan( self.ch2[batch].pos ).any(): raise ValueError('ch2 contains infinities. The RigidBody mapping likely exploded.')
        self.dev_mode('Rigid Body')
        
        
    ## Affine
    def Train_Affine(self, lr=1, Nit=200):
    # Training the Affine Mapping
        if self.AffineModel is not None: raise Exception('Models can only be trained once')
        if self.mid.all() != 0: print('WARNING! The image is not centered. This may have have detrimental effects for mapping a rotation!')
        
        # initializing the model and optimizer
        self.AffineModel=AffineModel(direct=self.coupled)
        
        # Training the Model
        print('Training Affine Mapping with (lr, #it) =',str((lr, Nit)),'...')
        # first train the A matrix (rot, shear, scaling)
        opt1=tf.optimizers.Adagrad(lr)
        self.AffineModel = self.train_model(self.AffineModel, Nit, opt1)
        
        # then train the d vector (shift)
        self.AffineModel.d._trainable=True
        self.AffineModel.A._trainable=False
        opt2=tf.optimizers.Adagrad(lr)
        self.AffineModel = self.train_model(self.AffineModel, Nit, opt2)
        
        
    
    def Transform_Affine(self):
    # Transforms ch2 according to the Model
        if self.AffineModel is None: print('Model not trained yet, will pass without transforming.')
        else:
            if self.mid.all() != 0: print('WARNING! The image is not centered. This may have have detrimental effects for mapping a rotation!')
            for batch in range(self.Nbatch):
                print('Transforming Affine Mapping...')
                ch2_tf=tf.Variable(self.ch2[batch].pos, dtype=tf.float32, trainable=False)
                ch2_tf=self.AffineModel.transform_vec(ch2_tf)
                self.ch2[batch].pos=np.array(ch2_tf.numpy())
                if np.isnan( self.ch2[batch].pos ).any(): raise ValueError('ch2 contains infinities. The Affine mapping likely exploded.')
        self.dev_mode('Affine')
      
        
    ## Polynomial3
    def Train_Polynomial3(self, lr=1, Nit=200):
    # Training the Polynomial3 Mapping
        if self.Polynomial3Model is not None: raise Exception('Models can only be trained once')
        
        # initializing the model and optimizer
        self.Polynomial3Model=Polynomial3Model(direct=self.coupled)
        opt=tf.optimizers.Adagrad(lr)
        
        # Training the Model
        print('Training Polynomial3 Mapping (lr, #it)=',str((lr, Nit)),'...')
        self.Polynomial3Model = self.train_model(self.Polynomial3Model, Nit, opt)
        
    
    def Transform_Polynomial3(self):
    # Transforms ch2 according to the Model
        print('Transforming Polynomial3 Mapping...')
        if self.Polynomial3Model is None: print('Model not trained yet, will pass without transforming.')
        else:
            for batch in range(self.Nbatch):
                ch2_tf=tf.Variable(self.ch2[batch].pos, dtype=tf.float32, trainable=False)
                ch2_tf=self.Polynomial3Model.transform_vec(ch2_tf)
                self.ch2[batch].pos=np.array(ch2_tf.numpy())
                if np.isnan( self.ch2[batch].pos ).any(): raise ValueError('ch2 contains infinities. The Polynomial3 mapping likely exploded.')
        self.dev_mode('Polynomial-3')
        
        
    #%% CatmullRom Splines
    def Train_Splines(self, lr=1, Nit=200, gridsize=1000, edge_grids=1):
    # Training the Splines Mapping. lr is the learningrate, Nit the number of iterations
    # gridsize the size of the Spline grids and edge_grids the number of gridpoints extra at the edge
        if self.SplinesModel is not None: raise Exception('Models can only be trained once')
        
        ## Generate the borders of the system
        (x1_min, x2_min, x1_max, x2_max) = ([],[],[],[])
        for batch in range(self.Nbatch):
            x1_min.append( tf.reduce_min(tf.floor(self.ch2[batch].pos[:,0])) )
            x2_min.append( tf.reduce_min(tf.floor(self.ch2[batch].pos[:,1])) )
            x1_max.append( tf.reduce_max(tf.floor(self.ch2[batch].pos[:,0])) )
            x2_max.append( tf.reduce_max(tf.floor(self.ch2[batch].pos[:,1])) )
        self.x1_min = np.min(x1_min) / gridsize
        self.x2_min = np.min(x2_min) / gridsize
        self.x1_max = np.max(x1_max) / gridsize
        self.x2_max = np.max(x2_max) / gridsize
                                
        ## Create grid
        self.edge_grids = edge_grids
        self.gridsize=gridsize
        x1_grid = tf.range(0, self.x1_max - self.x1_min + self.edge_grids + 2, dtype=tf.float32)
        x2_grid = tf.range(0, self.x2_max - self.x2_min + self.edge_grids + 2, dtype=tf.float32)
        self.ControlPoints = tf.stack(tf.meshgrid(x1_grid, x2_grid), axis=-1)
        
        ## Create Nearest Neighbours        
        (ch2_input, ch1_input) = ([],[])
        if self.coupled:
            for batch in range(self.Nbatch):
                ## Create variables normalized by gridsize
                ch2_input.append( tf.Variable( tf.stack([
                    self.ch2[batch].pos[:,0] / gridsize - self.x1_min + edge_grids,
                    self.ch2[batch].pos[:,1] / gridsize - self.x2_min + edge_grids
                    ], axis=-1), dtype=tf.float32, trainable=False) )
                ch1_input.append( tf.Variable( tf.stack([
                    self.ch1[batch].pos[:,0] / gridsize - self.x1_min + edge_grids,
                    self.ch1[batch].pos[:,1] / gridsize - self.x2_min + edge_grids
                    ], axis=-1), dtype=tf.float32, trainable=False) )
        else:
            for batch in range(self.Nbatch):
                ## Create variables normalized by gridsize
                ch2_input.append( tf.Variable( tf.stack([
                    self.NN2[batch] / gridsize - self.x1_min + edge_grids,
                    self.NN2[batch] / gridsize - self.x2_min + edge_grids
                    ], axis=-1), dtype=tf.float32, trainable=False) )
                ch1_input.append( tf.Variable( tf.stack([
                    self.NN1[batch] / gridsize - self.x1_min + edge_grids,
                    self.NN1[batch] / gridsize - self.x2_min + edge_grids
                    ], axis=-1), dtype=tf.float32, trainable=False) )
            
        ## initialize optimizer
        opt=tf.optimizers.Adagrad(lr)
        self.SplinesModel=CatmullRomSpline2D(self.ControlPoints, direct=self.coupled)
        
        ## Training the Model
        print('Training Splines Mapping (lr, #it, gridsize) =',str((lr, Nit, gridsize)),'...') 
        self.SplinesModel = self.train_model(self.SplinesModel, Nit, opt, ch1_input, ch2_input)
        self.ControlPoints=self.SplinesModel.ControlPoints
                
    
    def Transform_Splines(self):
    # Transforms ch2 according to the Model
        print('Transforming Splines Mapping...')
        if self.SplinesModel is None: print('Model not trained yet, will pass without transforming.')
        else:
            if self.gridsize is None: raise Exception('No Grid has been generated yet')
            
            ch2_input=[]
            for batch in range(self.Nbatch):
                    ## Create variables normalized by gridsize
                    ch2_input.append( tf.Variable( tf.stack([
                        self.ch2[batch].pos[:,0] / self.gridsize - self.x1_min + self.edge_grids,
                        self.ch2[batch].pos[:,1] / self.gridsize - self.x2_min + self.edge_grids
                        ], axis=-1), dtype=tf.float32, trainable=False) )
                
            # transform the new ch2 model
            for batch in range(self.Nbatch):
                ch2_mapped = np.array( self.SplinesModel.transform_vec(ch2_input[batch]) ) 
                self.ch2[batch].pos=np.stack([
                    (ch2_mapped[:,0] + self.x1_min - self.edge_grids) * self.gridsize,
                    (ch2_mapped[:,1] + self.x2_min - self.edge_grids) * self.gridsize          
                    ], axis=-1)
                if np.isnan( self.ch2[batch].pos ).any(): raise ValueError('ch2 contains infinities. The Splines mapping likely exploded.')
        self.dev_mode('Splines')